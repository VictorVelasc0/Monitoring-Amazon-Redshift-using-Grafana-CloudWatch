# Monitoreo de Amazon Redshift utilizando Grafana & Cloudwatch

> üá¨üáß [Read this article in English](README.en.md)

Si alguna vez has estado en contacto con alguna base de datos o sistema, seguro te has tenido la necesidad de saber como se est√°n comportando, y es que al igual que un auto entender los par√°metros que afectan el comportamiento tales como el rendimiento, la velocidad o el rango, nos ayudan a saber si algo est√° mal o si tenemos recuross para poder llegar de punto a a punto B.

He recopilado algunas ideas de monitoreo usando herramientas como Amazon Managed Grafana y Cloudwatch con alertas por Slack, esperando que pueda contribuir y ayudar a los dem√°s a tener algunas ideas de como monitorizar un Data warehouse:

En este art√≠culo cubriremos:
- [Un data warehouse sin monitoreo](#un-data-warehouse-sin-monitoreo)
- [Monitoreo y observabilidad](#monitoreo-y-observabilidad)
- [Herramientas de monitoreo en AWS](#herramientas-de-monitoreo-en-aws)
- [CloudWatch con Grafana, donde el monitoreo brilla](#cloudwatch-y-grafana-donde-el-monitoreo-brilla)
- [Arquitectura de monitoreo de data warehouse con Grafana](#arquitectura-de-monitoreo-de-data-warehouse-con-grafana)
- [Monitoreo de CPU del cluster](#monitoreo-de-cpu-del-cl√∫ster)
- [Alertas de CPU usando Slack](#alertas-de-cpu-usando-slack)
- [Conclusiones](#conclusiones)

# Un data warehouse sin monitoreo
Los primeros indicios fueron casi imperceptibles: una carga que tardaba un poco m√°s de lo normal, un DAG de dbt que se extend√≠a algunos minutos extra. Pero con el crecimiento del negocio, esos minutos se convirtieron en horas, y las horas en un problema que ya no pod√≠a ignorarse. Las cargas al data warehouse comenzaron a dispararse, los queries se quedaban suspendidas indefinidamente y los tiempos de ejecuci√≥n de los DAGs no dejaban de crecer. El requerimiento era claro: hab√≠a que optimizar.

<p align="center">
  <img src="resources/bart.png" alt="Meme blind bart" width="450" />
</p>

Investigando en la documentaci√≥n de AWS encontr√© distintas t√©cnicas que promet√≠an alivio "sort keys, distribution styles, compresiones" y la documentaci√≥n de dbt ofrec√≠a sus propias estrategias a trav√©s de distintas materializaciones. Sin embargo, una pregunta persist√≠a: ¬øsabemos d√≥nde est√° realmente el problema? ¬øQu√© procesos eran los m√°s ineficientes? ¬øEn qu√© horarios ocurr√≠an? ¬øQui√©nes los ejecutaban?
Diagnosticar un cl√∫ster enfermo sin m√©tricas es como intentar operar a un paciente con los ojos vendados: puedes tener la mejor t√©cnica del mundo, pero si no sabes d√≥nde duele, cualquier intervenci√≥n es un disparo al aire. Era evidente que no ten√≠amos visibilidad sobre las dolencias reales del cl√∫ster, y aplicar optimizaciones a ciegas solo nos conducir√≠a a soluciones de bajo impacto, imposibles de medir y a√∫n m√°s dif√≠ciles de justificar.


# Monitoreo y observabilidad
De acuerdo con AWS, el monitoreo se define como "el proceso de recopilar datos y generar informes sobre diferentes m√©tricas que definen el estado del sistema" (Amazon Web Services, s.f.). En t√©rminos pr√°cticos, esto se traduce en tener un mecanismo de obtenci√≥n y reporte de elementos clave, algo que en el mundo de los microservicios modernos resulta indispensable: cuando una aplicaci√≥n est√° compuesta por m√∫ltiples servicios interdependientes, medir el comportamiento de cada uno de ellos es el primer paso para entender la salud del sistema en su conjunto.

La observabilidad, por otro lado, va un paso m√°s all√°. Mientras el monitoreo nos dice qu√© est√° ocurriendo, la observabilidad nos permite entender el por qu√©, es un enfoque investigativo que utiliza los datos recopilados para encontrar la causa ra√≠z de los problemas (Amazon Web Services, s.f.). En arquitecturas de microservicios esto cobra especial relevancia, ya que una falla puede originarse en un servicio y manifestarse en otro completamente distinto.

<p align="center">
  <img src="resources/observability_vs_monitoring.png" alt="observability vs monitoring image"  width="750" />
</p>


Llevando estos conceptos a nuestro caso, el monitoreo deb√≠a capturar las m√©tricas clave que afectan el rendimiento del data warehouse; tiempos de ejecuci√≥n de queries, uso de memoria, concurrencia de conexiones,  mientras que la observabilidad nos dar√≠a el marco anal√≠tico para interpretar esos datos y seleccionar las t√©cnicas de optimizaci√≥n m√°s adecuadas para cada situaci√≥n.

# Herramientas de monitoreo en AWS
AWS ofrece un ecosistema robusto de herramientas de monitoreo, tanto nativas como gestionadas, que permiten tener visibilidad sobre el estado de los servicios en distintos niveles. A continuaci√≥n, las m√°s relevantes para una arquitectura de datos:

## CloudWatch
Es el servicio central de monitoreo y observabilidad de AWS. Recopila m√©tricas, logs y eventos de los recursos de la cuenta en tiempo real, y permite configurar alarmas que pueden desencadenar acciones autom√°ticas a trav√©s de EventBridge, funciones Lambda o notificaciones v√≠a SNS (Amazon Web Services, s.f.-a; Bhatt, 2023).
## X-Ray
Es el servicio de rastreo distribuido de AWS. Recopila datos de los servicios que componen una aplicaci√≥n y genera mapas de dependencias que permiten identificar cuellos de botella, errores y excepciones entre microservicios, lo que lo convierte en una herramienta especialmente valiosa para diagnosticar problemas que atraviesan m√∫ltiples componentes (Amazon Web Services, s.f.-b; Bhatt, 2023).
## CloudTrail
Proporciona una vista unificada de gobernanza, auditor√≠a y cumplimiento normativo de todos los eventos y llamadas a la API realizadas dentro de una cuenta de AWS. Permite rastrear qu√© recursos fueron creados o eliminados, qui√©n lo hizo y cu√°ndo. Todos los registros son inmutables, lo que garantiza la integridad del historial de actividad.
## DataDog
Aunque no es un servicio propio de AWS, DataDog es una de las plataformas SaaS de observabilidad y seguridad m√°s adoptadas en la industria. Permite construir dashboards interactivos que consolidan m√©tricas, logs y trazas de todos los microservicios y sistemas de una organizaci√≥n en un solo lugar (Datadog, s.f.).

## Amazon Managed Grafana
Es el servicio completamente administrado de AWS basado en Grafana de c√≥digo abierto. Permite consultar, correlacionar y visualizar m√©tricas, logs y trazas provenientes de m√∫ltiples fuentes de datos, incluyendo CloudWatch, X-Ray y servicios de terceros, sin necesidad de gestionar la infraestructura subyacente. Admite la creaci√≥n colaborativa de dashboards, alertas y control de acceso a nivel de equipo (Amazon Web Services, s.f.-c).

# CloudWatch y Grafana: donde el monitoreo brilla
Cada una de las herramientas anteriores resuelve un problema espec√≠fico de visibilidad, pero en la pr√°ctica surge un desaf√≠o recurrente: los datos est√°n dispersos. Al comenzar con el monitoreo del data warehouse, uno de los primeros obst√°culos fue precisamente ese: necesitaba cruzar en un solo lugar las m√©tricas de estado del cl√∫ster de Redshift CPU, latencia, conexiones activas, con datos internos como las queries en ejecuci√≥n y las alertas propias de performance.

<p align="center">
  <img src="resources/meme.png" alt="meme handshake cloudwatch and grafana" width="450"  />
</p>

La integraci√≥n de Amazon Managed Grafana con CloudWatch como fuente de datos en conjunto de la conexi√≥n de Redshift como data source resolvi√≥ esto de manera elegante. Al conectar ambos servicios, fue posible unificar en un solo dashboard tanto las m√©tricas expuestas por AWS como las tablas de sistema de Redshift, y adem√°s agregar una integraci√≥n con Slack para recibir notificaciones en tiempo real ante cualquier anomal√≠a de rendimiento. En los siguientes apartados veremos exactamente c√≥mo se construy√≥ esta arquitectura.

# Arquitectura de monitoreo de data warehouse con Grafana

La arquitectura se apoya en tres elementos esenciales que trabajan en conjunto: 
**Amazon Managed Grafana** como herramienta central de visualizaci√≥n y alertas, 
**Amazon CloudWatch** como fuente de m√©tricas del cl√∫ster, y el **plugin nativo 
de Redshift para Grafana** para consultar directamente las tablas de sistema del 
data warehouse.

<p align="center">
  <img src="resources/architecture_monitoring.png" alt="architecture diagram" width="750"  />
</p>


## 1. Configuraci√≥n de fuentes de datos en Grafana

El primer paso es conectar Grafana con sus dos fuentes de informaci√≥n. La primera 
es **CloudWatch**, que expone las m√©tricas de infraestructura del cl√∫ster de 
Redshift como `CPUUtilization`, `ReadThroughput` o `DatabaseConnections` y que 
se agrega directamente como data source nativo dentro de Grafana 
(Grafana Labs, s.f.-a; Amazon Web Services, s.f.-d). 

La segunda fuente es **Redshift** en s√≠ mismo, conectado mediante el plugin oficial 
`grafana-redshift-datasource`, que permite ejecutar queries SQL desde Grafana 
directamente contra las tablas de sistema del cl√∫ster. Para que esta conexi√≥n 
funcione, es necesario definir previamente un *access policy* en AWS con los 
permisos adecuados que autorice a Grafana a conectarse al cl√∫ster 
(Grafana Labs, s.f.-b).

## 2. Dashboards por tem√°tica

Con ambas fuentes conectadas, el siguiente paso es centralizar los datos en 
dashboards especializados por tem√°tica. Cada dashboard combina m√©tricas de 
CloudWatch y consultas a tablas de sistema de Redshift para ofrecer una vista 
completa de un aspecto espec√≠fico del cl√∫ster. En mi implementaci√≥n se definieron 
los siguientes dashboards:

- **CPU Usage**: seguimiento del uso de CPU por nodo a lo largo del tiempo.
- **Query Monitoring**: queries activas, tiempos de ejecuci√≥n y colas de espera.
- **Cluster Healthcheck**: estado general del cl√∫ster, conexiones y throughput.


<p align="center">
  <img src="resources/health.png" alt="health monitoring" width="750"  />
</p>

## 3. Alertas con Alertmanager y notificaciones en Slack

El √∫ltimo componente de la arquitectura es el sistema de alertas. Grafana incluye 
**Alertmanager**, que permite definir reglas de alerta basadas en cualquier m√©trica 
o query disponible en los data sources configurados.

Para conectar las alertas con Slack, primero se debe crear una aplicaci√≥n de Grafana 
dentro del workspace de Slack y obtener el webhook correspondiente. Con esto se 
agrega un nuevo *contact point* en Grafana de tipo Slack (Grafana Labs, s.f.-c). 
Posteriormente se define una *notification policy* que filtra las alertas usando 
el label `dwh`, asegurando que solo las alertas del data warehouse lleguen a ese 
canal.

<p align="center">
  <img src="resources/contact.png" alt="contact point ilustration" width="750" />
</p>

Como ejemplo concreto, se configur√≥ una alerta que se dispara cuando el uso de CPU 
del nodo l√≠der supera el **80% durante m√°s de 30 minutos**. Los par√°metros clave 
de evaluaci√≥n son:

- **Intervalo de evaluaci√≥n**: cada 30 minutos.
- **Per√≠odo de confirmaci√≥n**: 10 minutos (la condici√≥n debe cumplirse durante ese 
  tiempo antes de disparar la alerta).
- **Data source**: CloudWatch con la m√©trica `CPUUtilization` del cl√∫ster.

Para personalizar el formato de los mensajes enviados a Slack, Grafana permite 
configurar plantillas en los *Optional Slack settings* del contact point. 
El t√≠tulo y el cuerpo del mensaje se definen con la sintaxis de Go templates, 
lo que permite incluir √≠conos din√°micos seg√∫n la severidad de la alerta 
(`Critical`, `Warning`, `Info`), el nombre de la alerta, su estado actual, 
una descripci√≥n, detalles adicionales y un enlace directo al dashboard 
correspondiente en Grafana. Estos valores de descripci√≥n y detalles se 
completan en la secci√≥n **"Add details for your alert rule"** al momento de 
definir cada regla.

<p align="center">
  <img src="resources/alertconf.png" alt="alert configuration" width="750" />
</p>


## Monitoreo de CPU del cl√∫ster

El dashboard de CPU es probablemente el m√°s cr√≠tico de toda la arquitectura; 
es el primero que hay que revisar cuando llega una alerta y el que m√°s r√°pido 
permite tomar decisiones de acci√≥n. Se compone de tres elementos visuales 
complementarios entre s√≠.

El primero es una **gr√°fica de l√≠nea de tiempo** que muestra la m√©trica 
`CPUUtilization` de CloudWatch segmentada por nodo del cl√∫ster, lo que permite 
identificar no solo si el CPU est√° alto, sino *en qu√© nodo* se est√° concentrando 
la presi√≥n. El segundo son **indicadores** que muestran el estado m√°s reciente de CPU 
por nodo, √∫tiles para tener una lectura instant√°nea del momento actual sin tener 
que interpretar la gr√°fica hist√≥rica.

<p align="center">
  <img src="resources/cpu.png" alt="Cpu usage dashboard" width="750" />
</p>

El tercer elemento es una tabla de **queries activos en tiempo real**, construida 
sobre la tabla de sistema `STV_RECENTS` de Redshift, que registra las consultas 
en ejecuci√≥n en el cl√∫ster en este momento (Amazon Web Services, s.f.-e). 
La consulta utilizada es la siguiente:

```sql
SELECT
    pid,
    db_name                          AS "database",
    status,
    duration / 60000000.0            AS "query_duration_minutes",
    user_name,
    query,
    starttime
FROM stv_recents
WHERE status = 'Running';
```

Esta tabla es la pieza que convierte el dashboard de CPU en una herramienta 
de diagn√≥stico accionable: al cruzar el estado de CPU por nodo con los queries 
que est√°n ejecutando en ese momento, es posible identificar qu√© procesos est√°n 
agotando los recursos, qu√© usuarios o aplicaciones los est√°n generando y cu√°nto 
tiempo llevan corriendo. Con esa informaci√≥n, la acci√≥n correctiva puede ser 
tan inmediata como cancelar una consulta pesada que lleva horas saturando el 
nodo l√≠der, o tan estrat√©gica como priorizar ese proceso para una ronda de 
optimizaci√≥n.

### Worst CPU Queries

M√°s all√° del estado en tiempo real, es igual de valioso entender cu√°les han 
sido hist√≥ricamente los queries m√°s costosos en t√©rminos de CPU. Para esto se 
construy√≥ el panel **Worst CPU Queries**, que cruza tres tablas de sistema de 
Redshift:

- `STL_QUERY`: registro hist√≥rico de todas los queries ejecutados en el cl√∫ster 
  (Amazon Web Services, s.f.-f).
- `SVL_QUERY_METRICS`: vista que expone m√©tricas de rendimiento de queries 
  completadas, incluyendo `cpu_time`, `cpu_skew`, `io_skew` y conteos de filas 
  procesadas (Amazon Web Services, s.f.-g).
- `STL_ALERT_EVENT_LOG`: tabla que registra las alertas de rendimiento que 
  Redshift levanta internamente para un query, incluyendo el tipo de evento 
  y la sugerencia de soluci√≥n correspondiente (Amazon Web Services, s.f.-h).

<p align="center">
  <img src="resources/worst.png" alt="Worst CPU usage table"  width="950" />
</p>

La combinaci√≥n de estas tres fuentes es particularmente poderosa: no solo muestra 
cu√°nto CPU consumi√≥ un query, sino que incluye directamente la recomendaci√≥n de 
Redshift para solucionar el problema de rendimiento detectado. La consulta 
utilizada es:

```sql
SELECT 
    stq.pid,
    stq.query,
    u.usename                                   AS "username",
    LEFT(stq.querytxt, 500)                     AS "querytext",
    svq.cpu_skew,
    svq.io_skew,
    TRIM(SPLIT_PART(event, ':', 1))             AS "redshift_alert",
    TRIM(solution)                              AS "redshift_solution",
    svq.query_cpu_usage_percent,
    svq.query_cpu_time                          AS "query_cpu_time_seconds",
    svq.query_cpu_time / 60                     AS "query_cpu_time_minutes",
    svq.join_row_count                          AS "rows_joined",
    svq.scan_row_count,
    svq.query_execution_time,
    stq.starttime,
    stq.endtime
FROM stl_query stq
JOIN svl_query_metrics svq 
    ON stq.query = svq.query 
LEFT JOIN stl_alert_event_log ael 
    ON ael.query = stq.query
LEFT JOIN pg_user u 
    ON stq.userid = u.usesysid
WHERE svq.query_cpu_time IS NOT NULL 
  AND $__timeFilter(stq.starttime) 
ORDER BY svq.query_cpu_time DESC 
LIMIT 20;
```

## Alertas de CPU usando Slack

Con el dashboard de CPU construido, el sistema de alertas cierra el ciclo: en 
lugar de tener que revisar Grafana manualmente de forma peri√≥dica, la alerta 
act√∫a como un centinela que notifica el momento exacto en que el 
cl√∫ster est√° bajo presi√≥n.

La regla definida eval√∫a la m√©trica `CPUUtilization` de CloudWatch en el nodo 
l√≠der cada 30 minutos, y se dispara si el valor supera el **80% durante un 
per√≠odo de confirmaci√≥n de 10 minutos**, esto evita falsos positivos por 
picos moment√°neos. Cuando la condici√≥n se cumple, Grafana env√≠a una notificaci√≥n 
al canal de Slack correspondiente con el nivel de severidad, la descripci√≥n del 
problema y un enlace directo al dashboard de CPU para iniciar el diagn√≥stico.

<p align="center">
  <img src="resources/alert.png" alt="Grafana alert slack" width="450"/>
</p>

La respuesta ante una alerta puede seguir dos caminos dependiendo de lo que 
revele el dashboard: si hay un query que lleva horas en ejecuci√≥n y est√° 
saturando el nodo l√≠der, la acci√≥n m√°s inmediata es cancelarlo. Si el problema 
es recurrente y est√° asociado a un proceso propio, como un modelo de dbt con 
una materializaci√≥n ineficiente o una consulta sin sort key adecuado, la alerta 
se convierte en el punto de partida para una labor de optimizaci√≥n fundamentada 
en datos reales.

## Conclusiones

Implementar esta arquitectura de monitoreo fue, en muchos sentidos, como encender 
las luces en una habitaci√≥n que llevaba tiempo a oscuras. Los problemas no eran 
nuevos, los queries lentos, los DAGs que tardaban cada vez m√°s, el cl√∫ster que 
respond√≠a con fatiga todo esto ocurriendo sin m√©tricas, sin visibilidad, sin un lugar donde 
mirar, cualquier intento de soluci√≥n era poco m√°s que una apuesta.

Lo que cambi√≥ con el monitoreo no fue el cl√∫ster en s√≠, sino la capacidad 
de entenderlo. Se identificaron queries de testing de dbt que consum√≠an recursos 
en producci√≥n sin que nadie lo supiera. Se descubrieron usuarios cuyas consultas 
llevaban horas ejecut√°ndose y saturaban el nodo l√≠der. Se encontraron patrones 
recurrentes que, ahora visibles, se convirtieron en candidatos concretos para 
optimizaci√≥n.

El monitoreo de un data warehouse es uno de esos temas que se posterga 
indefinidamente "siempre hay una feature m√°s urgente, un pipeline m√°s prioritario" 
hasta que el sistema colapsa y la urgencia se impone. Sin embargo 
invertir en visibilidad antes de que surja el problema es exactamente lo que 
permite que el sistema escale con el negocio en lugar de convertirse en un 
cuello de botella.

Si hay algo que esta experiencia me dej√≥ claro, es que un data warehouse sin 
monitoreo no es un sistema que funciona bien: es un sistema que a√∫n no ha 
fallado lo suficiente como para que nos demos cuenta de que no lo entendemos.

# Referencias
- Amazon Web Services. (s.f.). The difference between monitoring and observability. AWS. https://aws.amazon.com/es/compare/the-difference-between-monitoring-and-observability/
- Amazon Web Services. (s.f.-a). Amazon CloudWatch. AWS. https://aws.amazon.com/es/cloudwatch/
- Amazon Web Services. (s.f.-b). AWS X-Ray. AWS. https://aws.amazon.com/es/xray/
- Amazon Web Services. (s.f.-c). Amazon Managed Grafana. AWS. https://aws.amazon.com/grafana/
- Bhatt, S. (2023). AWS Certified Data Engineer ‚Äì Associate [Curso en l√≠nea]. Udemy. https://www.udemy.com/course/
aws-data-engineer-associate/
- Datadog. (s.f.). The monitoring and security platform for cloud applications. https://www.datadoghq.com/
- Amazon Web Services. (s.f.-d). *Viewing cluster performance data ‚Äî Amazon Redshift*.  
https://docs.aws.amazon.com/redshift/latest/mgmt/performance-metrics-perf.html
- Grafana Labs. (s.f.-a). *Amazon CloudWatch data source*.  
https://grafana.com/docs/grafana/latest/datasources/aws-cloudwatch/
- Grafana Labs. (s.f.-b). *Redshift data source plugin*.  
https://grafana.com/docs/plugins/grafana-redshift-datasource/latest/
- Grafana Labs. (s.f.-c). *Configure Slack contact point*.  
https://grafana.com/docs/grafana/latest/alerting/configure-notifications/manage-contact-points/integrations/configure-slack/
- Amazon Web Services. (s.f.-e). *STV_RECENTS ‚Äî Amazon Redshift*.  
https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_STV_RECENTS.html
- Amazon Web Services. (s.f.-f). *STL_QUERY ‚Äî Amazon Redshift*.  
https://docs.aws.amazon.com/redshift/latest/dg/r_STL_QUERY.html
- Amazon Web Services. (s.f.-g). *SVL_QUERY_METRICS ‚Äî Amazon Redshift*.  
https://docs.aws.amazon.com/redshift/latest/dg/r_SVL_QUERY_METRICS.html
- Amazon Web Services. (s.f.-h). *STL_ALERT_EVENT_LOG ‚Äî Amazon Redshift*.  
https://docs.aws.amazon.com/redshift/latest/dg/r_STL_ALERT_EVENT_LOG.html